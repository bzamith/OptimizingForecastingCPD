{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Execute full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "from keras_tuner import RandomSearch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from config.constants import (\n",
    "    FORECAST_HORIZON, NB_TRIALS,\n",
    "    OBSERVATION_WINDOW, SEED, TRAIN_PERC\n",
    ")\n",
    "\n",
    "from src.change_point_detector import ChangePointCostFunction, ChangePointMethod, get_change_point_detector\n",
    "from src.dataset import read_dataset, split_X_y, split_train_test\n",
    "from src.forecaster import InternalForecaster, TimeSeriesHyperModel\n",
    "from src.scaler import Scaler\n",
    "from src.utils import get_error_results\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Is GPU available? [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Available devices:\", tf.config.list_physical_devices())\n",
    "print(\"Is GPU available?\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TensorFlow is using Apple GPU via MPS\n"
     ]
    }
   ],
   "source": [
    "if tf.config.list_physical_devices(\"GPU\"):\n",
    "    try:\n",
    "        tf.config.set_visible_devices([], \"GPU\")  # Hide traditional GPUs if any (like external ones)\n",
    "        tf.config.experimental.set_memory_growth(tf.config.list_physical_devices(\"GPU\")[0], True)\n",
    "        print(\"✅ TensorFlow is using Apple GPU via MPS\")\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Error setting MPS GPU:\", e)\n",
    "else:\n",
    "    print(\"❌ No GPU found, running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = 'validate_pipeline'\n",
    "dataset_domain_argv = 'UCI'\n",
    "dataset_argv = 'APPLIANCES_ENERGY'\n",
    "change_point_method_argv = 'Window'\n",
    "change_point_cost_function_argv = 'L1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_id = f\"{timestamp}_{dataset_domain_argv}_{dataset_argv}_{change_point_method_argv}_{change_point_cost_function_argv}_{SEED}\"\n",
    "change_point_method = ChangePointMethod.from_str(change_point_method_argv)\n",
    "change_point_cost_function = ChangePointCostFunction.from_str(change_point_cost_function_argv)\n",
    "change_point_approach = f\"{change_point_method.value.title()} {change_point_cost_function.value.title()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1] Reading dataset APPLIANCES_ENERGY from UCI\n",
      "Variables: ['T_out', 'Press_mm_hg', 'RH_out', 'Windspeed', 'Visibility', 'Tdewpoint']\n"
     ]
    }
   ],
   "source": [
    "print(f\"[Step 1] Reading dataset {dataset_argv} from {dataset_domain_argv}\")\n",
    "df, variables = read_dataset(dataset_domain_argv, dataset_argv)\n",
    "print(f\"Variables: {variables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 2] Splitting data into train_val and test\n"
     ]
    }
   ],
   "source": [
    "print(\"[Step 2] Splitting data into train_val and test\")\n",
    "train_val, test = split_train_test(df)\n",
    "report = {\n",
    "    'execution_id': execution_id,\n",
    "    'timestamp': timestamp,\n",
    "    'change_point_method': change_point_method.value,\n",
    "    'change_point_cost_function': change_point_cost_function.value,\n",
    "    'change_point_approach': change_point_approach,\n",
    "    'seed': SEED,\n",
    "    'observation_window': OBSERVATION_WINDOW,\n",
    "    'train_perc': TRAIN_PERC,\n",
    "    'nb_trials': NB_TRIALS,\n",
    "    'dataset_domain': dataset_domain_argv,\n",
    "    'dataset': dataset_argv,\n",
    "    'variables': variables,\n",
    "    'dataset_shape': df.shape,\n",
    "    'train_val_shape': train_val.shape,\n",
    "    'test_shape': test.shape,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 3] Detecting cut point (Window L1)\n",
      "Change point: 8460, Change point percentage: 53.5850012667849\n"
     ]
    }
   ],
   "source": [
    "print(f\"[Step 3] Detecting cut point ({change_point_approach})\")\n",
    "start_time = time.time()\n",
    "change_point_detector = get_change_point_detector(change_point_method, change_point_cost_function)\n",
    "change_point, change_point_perc = change_point_detector.find_change_point(train_val, variables)\n",
    "end_time = time.time()\n",
    "detect_change_point_duration = end_time - start_time\n",
    "print(f\"Change point: {change_point}, Change point percentage: {change_point_perc}\")\n",
    "report.update({\n",
    "    'detect_change_point_duration': detect_change_point_duration,\n",
    "    'change_point': str(change_point),\n",
    "    'change_point_perc': change_point_perc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 4] Reducing train_val based on change point\n"
     ]
    }
   ],
   "source": [
    "print(\"[Step 4] Reducing train_val based on change point\")\n",
    "start_time = time.time()\n",
    "reduced_train_val = change_point_detector.apply_change_point(train_val, change_point)\n",
    "end_time = time.time()\n",
    "apply_change_point_duration = end_time - start_time\n",
    "report.update({\n",
    "    'apply_change_point_duration': apply_change_point_duration,\n",
    "    'reduced_train_val.shape': reduced_train_val.shape,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 5] Splitting train_val into train and val\n"
     ]
    }
   ],
   "source": [
    "print(\"[Step 5] Splitting train_val into train and val\")\n",
    "reduced_train, reduced_val = split_train_test(reduced_train_val)\n",
    "report.update({\n",
    "    'reduced_train.shape': reduced_train.shape,\n",
    "    'reduced_val.shape': reduced_val.shape,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 6] Fitting scaler on train and applying on train and val\n"
     ]
    }
   ],
   "source": [
    "print(\"[Step 6] Fitting scaler on train and applying on train and val\")\n",
    "start_time = time.time()\n",
    "scaler = Scaler(variables)\n",
    "scaled_reduced_train = scaler.fit_scale(reduced_train)\n",
    "scaled_reduced_val = scaler.scale(reduced_val)\n",
    "end_time = time.time()\n",
    "fit_apply_scaler_train_val_duration = end_time - start_time\n",
    "report.update({\n",
    "    'fit_apply_scaler_train_val_duration': fit_apply_scaler_train_val_duration,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 7] Splitting train and val into X and y\n"
     ]
    }
   ],
   "source": [
    "print(\"[Step 7] Splitting train and val into X and y\")\n",
    "X_reduced_scaled_train, y_reduced_scaled_train = split_X_y(scaled_reduced_train)\n",
    "X_reduced_scaled_val, y_reduced_scaled_val = split_X_y(scaled_reduced_val)\n",
    "report.update({\n",
    "    'X_reduced_scaled_train.shape': X_reduced_scaled_train.shape,\n",
    "    'y_reduced_scaled_train.shape': y_reduced_scaled_train.shape,\n",
    "    'X_reduced_scaled_val.shape': X_reduced_scaled_val.shape,\n",
    "    'y_reduced_scaled_val.shape': y_reduced_scaled_val.shape,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.42508344,  0.34656565, -1.55042352, -1.21398052, -1.10125741,\n",
       "        -0.90743084],\n",
       "       [ 0.43823785,  0.34907256, -1.60052507, -1.21398052, -1.16634056,\n",
       "        -0.94966489],\n",
       "       [ 0.45139227,  0.35157947, -1.65062661, -1.21398052, -1.2314237 ,\n",
       "        -0.99189893],\n",
       "       [ 0.46454668,  0.35408636, -1.70072815, -1.21398052, -1.29650684,\n",
       "        -1.03061347],\n",
       "       [ 0.47770109,  0.35659326, -1.75082969, -1.21398052, -1.36158999,\n",
       "        -1.07284751],\n",
       "       [ 0.4908555 ,  0.36662088, -1.75082969, -1.21398052, -1.34857336,\n",
       "        -1.04926683],\n",
       "       [ 0.50400991,  0.37664847, -1.75082969, -1.21398052, -1.33555673,\n",
       "        -1.02709396]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_reduced_scaled_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.43823785,  0.34907256, -1.60052507, -1.21398052, -1.16634056,\n",
       "        -0.94966489],\n",
       "       [ 0.45139227,  0.35157947, -1.65062661, -1.21398052, -1.2314237 ,\n",
       "        -0.99189893],\n",
       "       [ 0.46454668,  0.35408636, -1.70072815, -1.21398052, -1.29650684,\n",
       "        -1.03061347],\n",
       "       [ 0.47770109,  0.35659326, -1.75082969, -1.21398052, -1.36158999,\n",
       "        -1.07284751],\n",
       "       [ 0.4908555 ,  0.36662088, -1.75082969, -1.21398052, -1.34857336,\n",
       "        -1.04926683],\n",
       "       [ 0.50400991,  0.37664847, -1.75082969, -1.21398052, -1.33555673,\n",
       "        -1.02709396],\n",
       "       [ 0.51716432,  0.38667608, -1.75082969, -1.21398052, -1.3225401 ,\n",
       "        -1.00245744]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_reduced_scaled_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.45139227,  0.35157947, -1.65062661, -1.21398052, -1.2314237 ,\n",
       "        -0.99189893],\n",
       "       [ 0.46454668,  0.35408636, -1.70072815, -1.21398052, -1.29650684,\n",
       "        -1.03061347],\n",
       "       [ 0.47770109,  0.35659326, -1.75082969, -1.21398052, -1.36158999,\n",
       "        -1.07284751],\n",
       "       [ 0.4908555 ,  0.36662088, -1.75082969, -1.21398052, -1.34857336,\n",
       "        -1.04926683],\n",
       "       [ 0.50400991,  0.37664847, -1.75082969, -1.21398052, -1.33555673,\n",
       "        -1.02709396],\n",
       "       [ 0.51716432,  0.38667608, -1.75082969, -1.21398052, -1.3225401 ,\n",
       "        -1.00245744],\n",
       "       [ 0.53031874,  0.39670369, -1.75082969, -1.21398052, -1.30952347,\n",
       "        -0.97782091]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_reduced_scaled_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 8] Running HPO and NAS\n",
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "2                 |2                 |num_layers\n",
      "64                |64                |units_0\n",
      "0.001             |0.001             |learning_rate\n",
      "\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 20:29:41.252273: W tensorflow/core/framework/dataset.cc:959] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    }
   ],
   "source": [
    "print(\"[Step 8] Running HPO and NAS\")\n",
    "n_variables = len(variables)\n",
    "forecaster_hypermodel = TimeSeriesHyperModel(\n",
    "    n_variables=n_variables\n",
    ")\n",
    "forecaster_tuner = RandomSearch(\n",
    "    forecaster_hypermodel,\n",
    "    objective='val_loss',\n",
    "    max_trials=3,\n",
    "    executions_per_trial=1,\n",
    "    directory=f\"outputs/tuner/delete_me\",\n",
    "    project_name=\"delete_me\",\n",
    "    seed=SEED,\n",
    "    overwrite=True,\n",
    "    distribution_strategy=tf.distribute.MirroredStrategy()\n",
    ")\n",
    "start_time = time.time()\n",
    "forecaster_tuner.search(\n",
    "    X_reduced_scaled_train,\n",
    "    y_reduced_scaled_train,\n",
    "    validation_data=(X_reduced_scaled_val, y_reduced_scaled_val),\n",
    "    shuffle=False,\n",
    ")\n",
    "end_time = time.time()\n",
    "tuner_duration = end_time - start_time\n",
    "report.update({\n",
    "    'tuner_duration': tuner_duration\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Step 9] Retrieving best model\")\n",
    "best_trial = forecaster_tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "best_forecaster_model = forecaster_tuner.get_best_models(num_models=1)[0]\n",
    "print(f\"Trial ID: {best_trial.trial_id}\")\n",
    "print(f\"Hyperparameters: {best_trial.hyperparameters.values}\")\n",
    "print(f\"Score: {best_trial.score}\")\n",
    "print(\"-\" * 40)\n",
    "best_forecaster_model.summary()\n",
    "best_forecaster_model = InternalForecaster(\n",
    "    best_forecaster_model,\n",
    "    len(variables),\n",
    "    best_trial.hyperparameters.values['batch_size'],\n",
    "    best_trial.hyperparameters.values['epochs'],\n",
    ")\n",
    "report.update({\n",
    "    'best_trial_id': best_trial.trial_id,\n",
    "    'best_trial_hyperparameters': best_trial.hyperparameters.values,\n",
    "    'best_trial_score': best_trial.score,\n",
    "    'best_forecaster_model': best_forecaster_model.summary(),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Step 10] Fitting scaler on train_val and applying on train_val and test\")\n",
    "start_time = time.time()\n",
    "scaler = Scaler(variables)\n",
    "scaled_reduced_train_val = scaler.fit_scale(reduced_train_val)\n",
    "scaled_test = scaler.scale(test)\n",
    "end_time = time.time()\n",
    "fit_apply_scaler_train_val_test_duration = end_time - start_time\n",
    "report.update({\n",
    "    'fit_apply_scaler_train_val_test_duration': fit_apply_scaler_train_val_test_duration,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Step 11] Splitting train_val and test into X and y\")\n",
    "X_reduced_scaled_train_val, y_reduced_scaled_train_val = split_X_y(scaled_reduced_train_val)\n",
    "X_scaled_test, y_scaled_test = split_X_y(scaled_test)\n",
    "report.update({\n",
    "    'X_reduced_scaled_train_val.shape': X_reduced_scaled_train_val.shape,\n",
    "    'y_reduced_scaled_train_val.shape': y_reduced_scaled_train_val.shape,\n",
    "    'X_scaled_test.shape': X_scaled_test.shape,\n",
    "    'y_scaled_test.shape': y_scaled_test.shape,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Step 12] Retraining best model\")\n",
    "start_time = time.time()\n",
    "best_forecaster_model.fit(\n",
    "    X_reduced_scaled_train_val,\n",
    "    y_reduced_scaled_train_val,\n",
    "    shuffle=False\n",
    ")\n",
    "end_time = time.time()\n",
    "retrain_duration = end_time - start_time\n",
    "report.update({\n",
    "    'retrain_duration': retrain_duration,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Step 13] Forecasting for test\")\n",
    "start_time = time.time()\n",
    "y_scaled_pred = best_forecaster_model.forecast(X_scaled_test)\n",
    "y_scaled_test_flat = y_scaled_test.reshape(-1, n_variables)\n",
    "y_scaled_pred_flat = y_scaled_pred.reshape(-1, n_variables)\n",
    "end_time = time.time()\n",
    "forecasting_test_duration = end_time - start_time\n",
    "report.update({\n",
    "    'forecasting_test_duration': forecasting_test_duration,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scaled_test_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scaled_pred_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Step 14] Descaling data\")\n",
    "start_time = time.time()\n",
    "y_test = scaler.descale(pd.DataFrame(y_scaled_test_flat, columns=variables))\n",
    "y_pred = scaler.descale(pd.DataFrame(y_scaled_pred_flat, columns=variables))\n",
    "end_time = time.time()\n",
    "descaling_duration = end_time - start_time\n",
    "report.update({\n",
    "    'descaling_duration': descaling_duration,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Step 15] Calculating evaluation metrics\")\n",
    "total_duration = sum(value for key, value in report.items() if key.endswith('_duration'))\n",
    "error_results = get_error_results(y_test, y_pred, variables)\n",
    "print(f\"Obtained error results: {error_results}\")\n",
    "report.update({\n",
    "    'total_duration': total_duration,\n",
    "    'error_results': error_results,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What would be the error if we predicted the average values for all variables (Dummy Forecaster)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, y_train_val = split_X_y(train_val)\n",
    "X_test, y_test = split_X_y(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_targets_flat = pd.DataFrame(y_train_val.reshape(-1, len(variables)), columns=variables)\n",
    "avg_values = train_val_targets_flat.mean(axis=0).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = y_test.shape[0]\n",
    "dummy_pred = np.tile(avg_values, (n_test, FORECAST_HORIZON, 1))\n",
    "\n",
    "dummy_pred_flat = dummy_pred.reshape(-1, n_variables)\n",
    "y_test_flat = pd.DataFrame(y_test.reshape(-1, n_variables), columns=variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_error_results = get_error_results(y_test_flat, dummy_pred_flat, variables)\n",
    "print(f\"Error metrics for Dummy Forecaster (predicting average values): \\n{dummy_error_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparison = pd.DataFrame({\n",
    "    \"Trained Model\": pd.Series(error_results),\n",
    "    \"Dummy Forecaster\": pd.Series(dummy_error_results)\n",
    "})\n",
    "\n",
    "df_comparison = df_comparison.round(5)\n",
    "df_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
